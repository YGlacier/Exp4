# Net of UniversalModel
# activation_blob: ip1
# hidden_layer: 512x2

# Input 256 units of hidden activations and 18 of action flags
layers {
  name: "input_layer"
  type: MEMORY_DATA
  top: "input_activation"
  top: "dummy1"
  memory_data_param {
    batch_size: 32
    channels: 274
    height: 1
    width: 1
  }
}

# Input 1s for non-terminal state, 0s for terminal state
layers {
  name: "filter_layer"
  type: MEMORY_DATA
  top: "filter"
  top: "dummy2"
  memory_data_param {
    batch_size: 32
    channels: 256
    height: 1
    width: 1
  }
}

# Target values: representations of next states
layers {
  name: "target_activation_layer"
  type: MEMORY_DATA
  top: "target_activation"
  top: "dummy3"
  memory_data_param {
    batch_size: 32
    channels: 256
    height: 1
    width: 1
  }
}

# Target reward
layers {
  name: "target_reward_layer"
  type: MEMORY_DATA
  top: "target_reward"
  top: "dummy4"
  memory_data_param {
    batch_size: 32
    channels: 1
    height: 1
    width: 1
  }
}

layers {
  name: "target_termination_layer"
  type: MEMORY_DATA
  top: "target_termination"
  top: "dummy5"
  memory_data_param {
    batch_size: 32
    channels: 1
    height: 1
    width: 1
  }
}

layers {
  name: "silence_layer"
  type: SILENCE
  bottom: "dummy1"
  bottom: "dummy2"
  bottom: "dummy3"
  bottom: "dummy4"
  bottom: "dummy5"
}

layers {
  name: "ip1_layer"
  type: INNER_PRODUCT
  bottom: "input_activation"
  top: "ip1"
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

layers {
  name: "ip1_relu_layer"
  type: RELU
  bottom: "ip1"
  top: "ip1"
  relu_param {
    negative_slope: 0.01
  }
}

layers {
  name: "ip2_layer"
  type: INNER_PRODUCT
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

layers {
  name: "ip2_relu_layer"
  type: RELU
  bottom: "ip2"
  top: "ip2"
  relu_param {
    negative_slope: 0.01
  }
}

layers {
  name: "ip3_layer"
  type: INNER_PRODUCT
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

layers {
  name: "ip3_relu_layer"
  type: RELU
  bottom: "ip3"
  top: "ip3"
  relu_param {
    negative_slope: 0.01
  }
}

layers {
  name: "output_layer"
  type: INNER_PRODUCT
  bottom: "ip3"
  top: "output"
  inner_product_param {
    num_output: 261
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

layers {
  name: "slice_layer"
  type: SLICE
  bottom: "output"
  top: "output_activation"
  top: "output_reward"
  top: "output_termination"
  slice_param {
    slice_dim: 1
    slice_point: 256
    slice_point: 259
  }
}

layers {
  name: "eltwise_layer"
  type: ELTWISE
  bottom: "output_activation"
  bottom: "filter"
  top: "filtered_output_activation"
  eltwise_param {
    operation: PROD
  }
}

layers {
  name: "activation_loss_layer"
  type: EUCLIDEAN_LOSS
  bottom: "filtered_output_activation"
  bottom: "target_activation"
  top: "activation_loss"
}

layers {
  name: "reward_loss_layer"
  type: SOFTMAX_LOSS
  bottom: "output_reward"
  bottom: "target_reward"
  top: "reward_loss"
}

layers {
  name: "termination_loss_layer"
  type: SOFTMAX_LOSS
  bottom: "output_termination"
  bottom: "target_termination"
  top: "termination_loss"
}
